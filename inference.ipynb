{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime \n",
    "import torch \n",
    "import torch \n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel \n",
    "\n",
    "\n",
    "# Note: not only GPT2 \n",
    "def format_time(elapsed):\n",
    "    # print nicely formated elapsed time\n",
    "    return str(datetime.timedelta(seconds=int(round((elapsed)))))\n",
    "\n",
    "class HitchensQuoteModel(): \n",
    "    \n",
    "    def __init__(self, model_input: GPT2LMHeadModel = None, device=\"cuda\"):\n",
    "        self.device = device\n",
    "        self.device = torch.device(self.device)  # Sloppily use Cuda GPU. \n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2', pad_token=\"<|endoftext|>\")  # extract the gpt2 tokenizer\n",
    "        self.model = self.init_model(model_input=model_input)  # initiate model\n",
    "        self.model.to(self.device)  # send model to device\n",
    "\n",
    "    def init_model(self, model_input: GPT2LMHeadModel) -> GPT2LMHeadModel:\n",
    "        \"\"\"\n",
    "            Load pretrained model or use input model\n",
    "        \"\"\"\n",
    "        if model_input is None:\n",
    "            return GPT2LMHeadModel.from_pretrained(\"gpt2\",\n",
    "                                                    pad_token_id=self.tokenizer.eos_token_id)\n",
    "        else:\n",
    "            return model_input\n",
    "    \n",
    "     \n",
    "    def generate_quotes(self,\n",
    "                        prompt=\"\",\n",
    "                        num_sentences: int = 3,\n",
    "                        max_length: int = 20,\n",
    "                        num_beams: int = 50,\n",
    "                        no_repeat_ngram_size: int = 3,\n",
    "                        print_it: bool = True,\n",
    "                        cuda: bool = True): \n",
    "        \n",
    "        if type(prompt) == str:\n",
    "            prompt = [prompt]\n",
    "            \n",
    "        input_ids = self.tokenizer(prompt,\n",
    "                                    return_tensors='pt',\n",
    "                                    padding=True,\n",
    "                                    truncation=True)[\"input_ids\"]\n",
    "\n",
    "        input_ids = input_ids.to(self.device)\n",
    "        output_temp = self.model.generate(\n",
    "            input_ids, \n",
    "            max_length=max_length, \n",
    "            num_beams=num_beams, \n",
    "            no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "            do_sample=True,\n",
    "            top_k=0,\n",
    "            num_return_sequences=num_sentences\n",
    "        )\n",
    "\n",
    "        if print_it:\n",
    "            for i in range(num_sentences):\n",
    "                print(self.tokenizer.decode(output_temp[i]))\n",
    "\n",
    "        self.model = self.model.to(self.device)\n",
    "        return output_temp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "------------------ ALL ------------------\n",
      "Trump is a icky, shady, demagogic nutbag...I and a few other people saw that he should be destroyed.<|endoftext|>\n",
      "\n",
      "\n",
      "------------------ ALL ------------------\n",
      "Clinton is a icky, arithmetical, pragmatic, pragmatic nutbag... I wouldn't have her job'. Those who profess unquenchable love for the sovereign are adamant that she press on in a task that they consider killingly hard.<|endoftext|>\n",
      "\n",
      "\n",
      "------------------ ALL ------------------\n",
      "Obama is a icky, arithmetical, pragmatic, pragmatic nutbag... I don't see how you can be saying this. Look. You want to take...you want your god to take responsibility for the huge number of collapsing stars and imploding galaxies and destroyed universes and failed solar systems that have left us in this tiny corner on the one planet in this petty solar system that can support life some of the time on some of its surface. And you want a creator who filled this earth with species, since life began 99% of which are now extinct already...and this is some design, isn't it?<|endoftext|>\n",
      "\n",
      "\n",
      "------------------ ALL ------------------\n",
      "Religion is a icky, arithmetical fact. It's impossible, I think, however much I'd become disillusioned politically or evolve into a post-political person, I don't think I'd ever change my view that God is man-made. The very idea that God speaks to some illiterate merchant warlord in Arabia, and he’s able to write this down perfectly and it contains the answers to all — don’t waste my time with that bulls**t. Also, the archangel Gabriel speaks only Arabic, it seems? Crap.<|endoftext|>\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([[ 6892, 17035,   318,   257,   220, 17479,    11,   610,   342,  4164,\n           605,  1109,    13,   632,   338,  5340,    11,   314,   892,    11,\n          2158,   881,   314,  1549,  1716, 39977,   276, 11889,   393, 18101,\n           656,   257,  1281,    12, 23149,  1048,    11,   314,   836,   470,\n           892,   314,  1549,  1683,  1487,   616,  1570,   326,  1793,   318,\n           582,    12,  9727,    13,   383,   845,  2126,   326,  1793,  9209,\n           284,   617, 45029,   378, 20729,  1175, 10572,   287,  9671,    11,\n           290,   339,   447,   247,    82,  1498,   284,  3551,   428,   866,\n          7138,   290,   340,  4909,   262,  7429,   284,   477,   851,   836,\n           447,   247,    83,  7030,   616,   640,   351,   326, 40317,  1174,\n            83,    13,  4418,    11,   262,  3934,  8368, 17371,  9209,   691,\n         17526,    11,   340,  2331,    30,   327,  2416,    13, 50256]],\n       device='cuda:0')"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model =  GPT2LMHeadModel.from_pretrained(\"hitch_gpt2\", local_files_only=True  )\n",
    " \n",
    "\n",
    "mod = HitchensQuoteModel(model_input=model)\n",
    "\n",
    "\n",
    "# predict with prompt\n",
    "prompt = \"Trump is a \" \n",
    "print(\"\\n\\n------------------ ALL ------------------\")\n",
    "mod.generate_quotes(prompt, max_length=200, num_sentences=1) \n",
    "# predict with prompt\n",
    "prompt = \"Clinton is a \" \n",
    "print(\"\\n\\n------------------ ALL ------------------\")\n",
    "mod.generate_quotes(prompt, max_length=200, num_sentences=1) \n",
    "\n",
    "prompt = \"Obama is a \" \n",
    "print(\"\\n\\n------------------ ALL ------------------\")\n",
    "mod.generate_quotes(prompt, max_length=200, num_sentences=1) \n",
    "\n",
    "prompt = \"Religion is a \" \n",
    "print(\"\\n\\n------------------ ALL ------------------\")\n",
    "mod.generate_quotes(prompt, max_length=200, num_sentences=1) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('venv': venv)",
   "name": "python385jvsc74a57bd0b8b57dfceea31b20adcfc17619e0dec71827d1de19fedaa08a84abb5b8410874"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}