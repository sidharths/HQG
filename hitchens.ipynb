{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, get_linear_schedule_with_warmup, AdamW\n",
    "\n",
    "\n",
    "# Note: not only GPT2\n",
    "\n",
    "def format_time(elapsed):\n",
    "  # print nicely formated elapsed time\n",
    "  return str(datetime.timedelta(seconds=int(round((elapsed)))))\n",
    "\n",
    "\n",
    "class GPT2Dataset(Dataset):\n",
    "  \"\"\"\n",
    "    Pytorch Dataset wrapper that helps with training and batches of training data. Reads in texts of politicians\n",
    "    :param txt_list: (Numpy) array of quotes.\n",
    "  \"\"\"\n",
    "  def __init__(self, txt_list, tokenizer, max_length=768):\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_length=max_length\n",
    "    self.input_ids = []\n",
    "    self.attn_masks = []\n",
    "\n",
    "    for txt in txt_list:\n",
    "      encodings_dict = tokenizer(txt, truncation=True, max_length=self.max_length, padding=\"max_length\")\n",
    "      self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
    "      self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
    "    \n",
    "  def __len__(self):\n",
    "    return len(self.input_ids)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.input_ids[idx], self.attn_masks[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetData():\n",
    "  \"\"\"\n",
    "  Retrieve dataset from local or google colab. Helps generate pytorch dataloaders.\n",
    "  :param data_path: path to Hitchens_quotes file\n",
    "  :param token_length: Max token length to use in GPT2 model\n",
    "  :param drive: Boolean if google drive should be mounted (used in google colab)\n",
    "  \"\"\"\n",
    "  def __init__(self, data_path, token_length=768, drive=False):\n",
    "    self.data_path = data_path\n",
    "    self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2', pad_token=\"<|endoftext|>\")\n",
    "    if drive:\n",
    "      self.connect_drive() # used for working on colab\n",
    "    self.quotes = self.read_data()\n",
    "    self.token_length = token_length\n",
    "\n",
    "    # after run once, keep as batch\n",
    "    self.dataset = None\n",
    "    self.dataloader = None\n",
    "\n",
    "  def connect_drive(self):\n",
    "    \"\"\"\n",
    "      Connect to Google Drive Data if drive in path\n",
    "    \"\"\"\n",
    "    if \"/content/drive\" in self.path:\n",
    "      try:\n",
    "          from google.colab import drive\n",
    "          COLAB = True\n",
    "          drive.mount('/content/drive')\n",
    "      except:\n",
    "        COLAB = False\n",
    "\n",
    "  def read_data(self):\n",
    "    \"\"\"\n",
    "      Read data as pandas dataframe\n",
    "    \"\"\"\n",
    "    file1 = open(self.data_path, 'r')\n",
    "    lines = [x[:-1] for x in file1.readlines()]\n",
    "    return lines\n",
    "\n",
    "  def get_data(self):\n",
    "    \"\"\"\n",
    "      Return the dataset\n",
    "    \"\"\"\n",
    "    return self.quotes\n",
    "\n",
    "  def get_dataset(self, quote_list: list = None):\n",
    "    \"\"\"\n",
    "      Create a custom pytorch dataset specialised for this task. A customer quote list can be used as well.\n",
    "    \"\"\"\n",
    "    if quote_list is None:\n",
    "      quote_list = self.get_data()\n",
    "\n",
    "    self.dataset = GPT2Dataset(\n",
    "        txt_list=quote_list,\n",
    "        tokenizer=self.tokenizer,\n",
    "        max_length=self.token_length\n",
    "    )\n",
    "    \n",
    "    return self.dataset\n",
    "\n",
    "  def get_dataloader(self, dataset: Dataset = None, batch_size=2):\n",
    "    \"\"\"\n",
    "      Create an iterable dataloader based on a GPT2Dataset and specified batch_size. Attention, a large batchsize quickly leads\n",
    "      to memory overloads.\n",
    "      :param dataset: Input GPT2Dataset object\n",
    "      :param batch_size: Integer of desired batch_size. Smaller equal 2 recommended.\n",
    "    \"\"\"\n",
    "    if dataset is None:\n",
    "      if self.dataset is None:\n",
    "        dataset = self.get_dataset()\n",
    "      else:\n",
    "        dataset = self.dataset\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        sampler=RandomSampler(dataset), # Select batches randomly\n",
    "        batch_size=batch_size # Trains with this batch size.\n",
    "    )\n",
    "    self.dataloader = dataloader\n",
    "    return dataloader\n",
    "\n",
    "  def get_data_of(self,\n",
    "                  Hitchens_name: str = None,\n",
    "                  after_year: int = None):\n",
    "    \"\"\"\n",
    "      Easy filter wrapper of data. Specify the Hitchens and years of desired data.\n",
    "      :param Hitchens_name: name of Hitchens\n",
    "      :param after_year: choose only quotes of thisyear and later only\n",
    "    \"\"\"\n",
    "    data_temp = self.get_data()\n",
    "    if Hitchens_name is not None:\n",
    "      data_temp = data_temp[data_temp.Hitchens_name == Hitchens_name]\n",
    "    if after_year is not None:\n",
    "      data_temp = data_temp[data_temp.year >= after_year]\n",
    "    return data_temp\n",
    "\n",
    "  def get_dataset_of(self, *args, **kwargs):\n",
    "    \"\"\"\n",
    "      Returns a custom pytorch dataset (GPT2Dataset) using a subset of data. See get_data_of(...)\n",
    "    \"\"\"\n",
    "    data_temp = self.get_data_of(**kwargs)\n",
    "    dataset_temp = self.get_dataset(quote_list=data_temp.quote.tolist())\n",
    "    return dataset_temp\n",
    "\n",
    "  def get_dataloader_of(self,\n",
    "                        Hitchens_name: str = None,\n",
    "                        after_year: int = None,\n",
    "                        batch_size=2):\n",
    "    \"\"\"\n",
    "      Return dataloader based on a subset of the data. See get_data_of(...)\n",
    "    \"\"\"\n",
    "    dataset_temp = self.get_dataset_of(Hitchens_name=Hitchens_name,\n",
    "                                       after_year=after_year)\n",
    "    dataloader_temp = self.get_dataloader(dataset=dataset_temp, batch_size=batch_size)\n",
    "    return dataloader_temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HitchensQuoteModel():\n",
    "  \"\"\"\n",
    "    Wrapper to easily train and finetune models based on Hitchens quote data.\n",
    "    :param model_input: Initially empty. Can be used to introduce a pretrained model.\n",
    "    :param device: Specify device to use\n",
    "  \"\"\"\n",
    "  def __init__(self, model_input: GPT2LMHeadModel = None, device=\"cuda\"):\n",
    "    self.device = device\n",
    "    self.device = torch.device(self.device)  # Sloppily use Cuda GPU. \n",
    "    self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2', pad_token=\"<|endoftext|>\")  # extract the gpt2 tokenizer\n",
    "    self.model = self.init_model(model_input=model_input)  # initiate model\n",
    "    self.model.to(self.device)  # send model to device\n",
    "\n",
    "  def init_model(self, model_input: GPT2LMHeadModel) -> GPT2LMHeadModel:\n",
    "    \"\"\"\n",
    "      Load pretrained model or use input model\n",
    "    \"\"\"\n",
    "    if model_input is None:\n",
    "      return GPT2LMHeadModel.from_pretrained(\"gpt2\",\n",
    "                                             pad_token_id=self.tokenizer.eos_token_id)\n",
    "    else:\n",
    "      return model_input\n",
    "\n",
    "  def fine_tune(self,\n",
    "                data,\n",
    "                epochs: int = 7,\n",
    "                learning_rate: float = 5e-5,\n",
    "                epsilon: float = 1e-8,\n",
    "                warmup_steps: int = 100) -> None:\n",
    "    \"\"\"\n",
    "      Simplify warm-up steps to finetune the model based on some data input.\n",
    "      :param data: A dataloader. Easily retrieved using get_dataloader method of GetData class\n",
    "      :param epochs: Integer, number of epochs\n",
    "      :param learning_rate: Learning Rate used in Adam Optimizer\n",
    "      :param epsilon: Epsilon used in Adam Optimizer\n",
    "      :param warmup_steps: Number of warmup steps in a linear schedule\n",
    "    \"\"\"\n",
    "    # check input\n",
    "    assert type(data) == DataLoader, \"Datatype for 'data' must be DataLoader\"\n",
    "\n",
    "    # define implicite variables\n",
    "    batch_size = data.batch_size\n",
    "    total_steps = len(data) * epochs\n",
    "    sample_every = 20\n",
    "\n",
    "    # define optimizer\n",
    "    optimizer = AdamW(self.model.parameters(),\n",
    "                      lr=learning_rate,\n",
    "                      eps=epsilon)\n",
    "\n",
    "    # define scheduler for learningrate strategy\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                num_warmup_steps=warmup_steps, \n",
    "                                                num_training_steps=total_steps)\n",
    "\n",
    "    self.model.train()  # put model in training mode (for dropout etc.)\n",
    "    for epoch_i in range(0, epochs):\n",
    "        t0 = time.time()\n",
    "        for step, batch in enumerate(data):\n",
    "            # set batch values\n",
    "            b_input_ids = batch[0].to(self.device)\n",
    "            b_labels = batch[0].to(self.device)\n",
    "            b_masks = batch[1].to(self.device) \n",
    "            self.model.zero_grad() # reset gradients to not accumulate! \n",
    "\n",
    "            # forward propagation\n",
    "            outputs = self.model.forward( \n",
    "                b_input_ids,\n",
    "                labels=b_labels, \n",
    "                #attention_mask = b_masks,\n",
    "                token_type_ids=None)\n",
    "\n",
    "            # update params\n",
    "            loss = outputs[0]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            batch_loss = loss.item()\n",
    "\n",
    "            # print in-between times\n",
    "            if step % sample_every == 0 and not step == 0:\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "                print('  Batch {:>5,}  of  {:>5,}. Loss: {:>5,}.   Elapsed: {:}.'.format(step, len(data), round(batch_loss, 4), elapsed))\n",
    "\n",
    "\n",
    "  def generate_sentences(self,\n",
    "                         prompt=\"\",\n",
    "                         num_sentences: int = 3,\n",
    "                         max_length: int = 20,\n",
    "                         num_beams: int = 50,\n",
    "                         no_repeat_ngram_size: int = 3,\n",
    "                         print_it: bool = True,\n",
    "                         cuda: bool = True):\n",
    "    \"\"\"\n",
    "      Easily generate sentences using the model (based on a prompt).\n",
    "    \"\"\"\n",
    "    if type(prompt) == str:\n",
    "      prompt = [prompt]\n",
    "    input_ids = self.tokenizer(prompt,\n",
    "                                return_tensors='pt',\n",
    "                                padding=True,\n",
    "                                truncation=True)[\"input_ids\"]\n",
    "\n",
    "    input_ids = input_ids.to(self.device)\n",
    "    output_temp = self.model.generate(\n",
    "        input_ids, \n",
    "        max_length=max_length, \n",
    "        num_beams=num_beams, \n",
    "        no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "        do_sample=True,\n",
    "        top_k=0,\n",
    "        num_return_sequences=num_sentences\n",
    "    )\n",
    "    if print_it:\n",
    "      for i in range(num_sentences):\n",
    "        print(self.tokenizer.decode(output_temp[i]))\n",
    "\n",
    "    self.model = self.model.to(self.device)\n",
    "    return output_temp\n",
    "\n",
    "  def copy_model(self):\n",
    "    \"\"\"\n",
    "      Return a copy of the model\n",
    "    \"\"\"\n",
    "    return deepcopy(self.model)\n",
    "\n",
    "  def copy(self):\n",
    "    return deepcopy(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_of_quotes = \"./quotes.txt\"\n",
    "dt = GetData(data_path=path_of_quotes)\n",
    "\n",
    "mod = HitchensQuoteModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_loader = dt.get_dataloader()\n",
    "# train on all Hitchenss\n",
    "params = dict(\n",
    "    epochs=20,\n",
    "    learning_rate=2e-4,\n",
    "    epsilon = 1e-07\n",
    ")\n",
    "\n",
    "mod.fine_tune(all_loader, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "prompt = \"Religion is\"\n",
    "temp = []\n",
    "print(\"\\n\\n------------------ ALL ------------------\")\n",
    "mod.generate_sentences(prompt, max_length=200, num_sentences=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('usr')",
   "name": "python382jvsc74a57bd05edc29c2ed010d6458d71a83433b383a96a8cbd3efe8531bc90c4b8a5b8bcec9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "metadata": {
   "interpreter": {
    "hash": "5edc29c2ed010d6458d71a83433b383a96a8cbd3efe8531bc90c4b8a5b8bcec9"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}